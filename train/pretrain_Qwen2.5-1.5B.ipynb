{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9102c3-5b8d-4295-8f29-113b35ec5679",
   "metadata": {},
   "source": [
    "# 一、LLM 预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557a6a6-294a-49c3-a8f6-e58bc3bf443d",
   "metadata": {},
   "source": [
    "1.1 初始化 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25f1fad8-772c-474e-a43e-77623106485d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"architectures\": [\n",
       "    \"Qwen2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"dtype\": \"bfloat16\",\n",
       "  \"eos_token_id\": 151643,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 1536,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8960,\n",
       "  \"layer_types\": [\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\",\n",
       "    \"full_attention\"\n",
       "  ],\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"max_window_layers\": 28,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 28,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"transformers_version\": \"4.57.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_mrope\": false,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 151936\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载定义好的模型参数-此处以 Qwen-2.5-1.5B 为例\n",
    "# 使用 transforemrs 的 Config 类进行加载\n",
    "from transformers import AutoConfig\n",
    "\n",
    "model_path = \"/root/projects/happy-llm/ZeroLLM/autodl-tmp/model/Qwen2.5-1.5B\"\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82b075a1-4fe9-4abb-b5b4-769d1c1a7156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model from scratch - Total size=1472.20M params\n"
     ]
    }
   ],
   "source": [
    "# 使用该配置生成一个定义好的模型\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_config(config,trust_remote_code=True)\n",
    "model.to(\"cuda\")\n",
    "n_params = sum({p.data_ptr(): p.numel() for p in model.parameters()}.values())\n",
    "print(f\"Training new model from scratch - Total size={n_params/2**20:.2f}M params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e05ea707-23db-4e67-8b7d-e57d019887dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看一下模型\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3408137b-eb50-4119-be1c-7a4ff951ab24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='/root/projects/happy-llm/ZeroLLM/autodl-tmp/model/Qwen2.5-1.5B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载一个预训练好的 tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a0fe2-a244-4e73-b82c-6da255d710dd",
   "metadata": {},
   "source": [
    "1.2 预训练数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "936261a6-94cf-4cf3-842c-d3f1fde47a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练数据\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('json', data_files='/root/projects/happy-llm/ZeroLLM/autodl-tmp/seq_monkey_datawhale_small.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "068edbb9-cb3c-49b1-aaf9-67b97ddfc58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '在查处虚开增值税专用发票案件中，常常涉及进项留抵税额和税款损失的认定和处理。在计算税款损失时，要不要将进项留抵税额包括在内？\\n对此，实务中存在意见分歧。\\n有人主张归并，即计算税款损失时包括进项留抵税额；\\n有人主张剥离，即计算税款损失时剔除进项留抵税额。分析这个问题，需要确定进项留抵税额与税款损失之间是什么关系。\\n理清这二者之间的关系，首先需要了解增值税的概念和其抵扣机制。增值税是以商品（货物、服务等）在流转过程中产生的增值额作为计税依据而征收的一种流转税。为避免重复征税，在增值税中存在抵扣链条机制。\\n一般而言，交易上游企业缴纳的税额，交易下游企业可以对相应的税额进行抵扣。\\n对增值税一般纳税人来说，其购进货物、服务等取得增值税专用发票，发票上的税额是进项税额。\\n其出售货物、服务等，向购买方开具增值税专用发票，发票的税额是销项税额。\\n一般情况下，销项税额减去进项税额的金额是应纳税额，企业根据应纳税额按期申报纳税。\\n其次需要了解进项留抵税额的概念及产生原因。\\n在计算销项税额和进项税额的差额时，有时会出现负数，即当期进项税额大于当期销项税额。这个差额在当期未实现抵扣，为进项留抵税额，在以后纳税人有销项税额时再进行抵扣'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef372a1f-e82f-4f5d-8495-f21f06b35635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n"
     ]
    }
   ],
   "source": [
    "# 查看特征\n",
    "column_names = list(ds[\"train\"].features)\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fa637f5-3b23-4a33-b19b-4c90d1815c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对数据集进行 tokenize\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # 使用预先加载的 tokenizer 进行分词\n",
    "    output = tokenizer([item for item in examples[\"text\"]])\n",
    "    return output\n",
    "\n",
    "# 批量处理\n",
    "tokenized_datasets = ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=10,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec30197e-fe7f-4f0d-903c-663146421f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ec5431b-e3cf-44e0-9260-479d984253e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预训练一般将文本拼接成固定长度的文本段\n",
    "from itertools import chain\n",
    "\n",
    "# 这里我们取块长为 2048\n",
    "block_size = 2048\n",
    "\n",
    "def group_texts(examples):\n",
    "    # 将文本段拼接起来\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    # 计算拼起来的整体长度\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # 如果长度太长，进行分块\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))       \n",
    "    print(\"group texts input examples length%d after_group size%d\"%(len(examples['input_ids']),len(result[\"input_ids\"])))\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38428a53-6ba6-429f-8c4b-0985579e726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量处理\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=10,\n",
    "    load_from_cache_file=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    batch_size = 40000,\n",
    ")\n",
    "train_dataset = lm_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0dd8c8-fb1f-4af9-8af4-21285ba389c0",
   "metadata": {},
   "source": [
    "1.3 使用 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3e1a85e-fc28-4154-870e-f6a09f108059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "# 配置训练参数\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"autodl-tmp/output/pretrain\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100, \n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62a97e46-ff06-4278-b318-e3e4da1b93d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64870/724298357.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, default_data_collator\n",
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "\n",
    "# 训练器\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= IterableWrapper(train_dataset),\n",
    "    eval_dataset= None,\n",
    "    tokenizer=tokenizer,\n",
    "    # 默认为 MLM 的 collator，使用 CLM 的 collater\n",
    "    data_collator=default_data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a929b11a-99f5-45fc-9f9a-05c0163204c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='783' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8/783 00:25 < 55:04, 0.23 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mstart train\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/transformers/trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/data_loader.py:874\u001b[39m, in \u001b[36mDataLoaderDispatcher.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.process_index != \u001b[32m0\u001b[39m:\n\u001b[32m    872\u001b[39m     \u001b[38;5;66;03m# Initialize tensors on other processes than process 0.\u001b[39;00m\n\u001b[32m    873\u001b[39m     batch = initialize_tensors(batch_info[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m874\u001b[39m batch = \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[38;5;66;03m# Broadcast the batch before splitting it.\u001b[39;00m\n\u001b[32m    876\u001b[39m batch = broadcast(batch, from_process=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/utils/operations.py:180\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    177\u001b[39m         skip_keys = []\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[32m    179\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor.items()\n\u001b[32m    182\u001b[39m         }\n\u001b[32m    183\u001b[39m     )\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/utils/operations.py:154\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    152\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mnpu:0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor.to(device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print('start train')\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed2cd9-7169-4376-a26c-053918074761",
   "metadata": {},
   "source": [
    "# 二、模型 SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6e02b-c04c-45a4-b36c-904f9fedf61e",
   "metadata": {},
   "source": [
    "2.1 处理指令数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d7cd012-fa2d-4c21-b6a5-c3830d12f59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'human',\n",
       "   'value': '针对健身房的新手，设计一套适合他们的健身器械使用指南，包括安全应用、正确姿势等方面。'},\n",
       "  {'from': 'assistant',\n",
       "   'value': '健身器械使用指南\\n1. 开始前，请先进行热身运动。这会帮助你的身体适应运动，并减少受伤的风险。\\n2. 在使用健身器械前，确保你已经了解了其使用方法。请阅读说明书或咨询教练以获得正确的使用技巧。\\n3. 谨防过度使用或过度挑战你的身体。 如果你觉得有些动作太难或太重，请添加锻炼计划，以逐步提高动作难度。\\n4. 使用合适的装备。 确保你拥有合适的运动鞋和舒适的运动服。 不要在裸露的脚或短裤上进行重量训练。\\n5. 在健身器械上使用安全装置。 这些通常用于保护你的身体免受不当操作造成的损伤。 例如，重量训练中，你需要使用杠铃和负重时，一定要使用卡子来防止重量滑落。\\n6. 注意正确的姿势。 如果你的姿势是错误的，那么你的身体很容易被伤害到，你也可能无法获得最佳的锻炼效果。 至关重要的是，保持直立的身体，保持头部和颈部的稳定，并使用合适的重量。\\n7. 保持合理的呼吸方式。 无论何时进行训练，都必须保持正常呼吸。 当你需要用力时，呼气； 当你放松时，吸气。\\n8. 安全存放器械。 在使用健身器械后，你需要把它们归还给适当的位置，以便其他人可以使用它们。\\n总之，健身器械的正确使用是关键之一，如果不健康和不安全，它们将无法帮助您达到您所需的健康成果。 选择适当的训练计划，并为训练提供足够的时间，以备逐渐适应新方法。 对于任何问题，请向教练咨询。'}],\n",
       " 'id': '66182880'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/root/projects/happy-llm/ZeroLLM/autodl-tmp/dataset/BelleGroup/train_3.5M_CN.json\") as f:\n",
    "    lst = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "lst[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8c599-89e9-4c35-a011-d2e52a1a4d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='/root/projects/happy-llm/ZeroLLM/autodl-tmp/model/Qwen2.5-1.5B', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 加载一个预训练好的 tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/root/projects/happy-llm/ZeroLLM/autodl-tmp/model/Qwen2.5-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46730b29-41c0-4295-81f2-913d069b4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 指令文本处理\n",
    "# 参考：https://github.com/QwenLM/Qwen/blob/main/finetune.py\n",
    "def preprocess(sources, tokenizer, max_len, system_message: str = \"You are a helpful assistant.\"):\n",
    "    # prompt 模板\n",
    "    roles = {\"human\": \"<|im_start|>human\", \"assistant\": \"<|im_start|>assistant\"}\n",
    "\n",
    "    # 不同的 tokenizer 需要特别定义\n",
    "    # BOS\n",
    "    im_start = tokenizer(\"<|im_start|>\").input_ids\n",
    "    # EOS\n",
    "    im_end = tokenizer(\"<|im_end|>\").input_ids\n",
    "    # PAD\n",
    "    IGNORE_TOKEN_ID = tokenizer.pad_token_id\n",
    "    # 换行符\n",
    "    nl_tokens = tokenizer('\\n').input_ids\n",
    "    # 角色标识符\n",
    "    _system = tokenizer('system').input_ids + nl_tokens\n",
    "    _user = tokenizer('human').input_ids + nl_tokens\n",
    "    _assistant = tokenizer('assistant').input_ids + nl_tokens\n",
    "\n",
    "    # 拼接多轮对话\n",
    "    input_ids, targets = [], []\n",
    "    for i in tqdm(range(len(sources))):\n",
    "        source = sources[i]\n",
    "        # 从 user 开始\n",
    "        if source[0][\"from\"] != \"human\":\n",
    "            source = source[1:]\n",
    "        # 分别是输入和输出\n",
    "        input_id, target = [], []\n",
    "        # system: 【BOS】system\\nYou are a helpful assistant.【EOS】\\n\n",
    "        system = im_start + _system + tokenizer(system_message).input_ids + im_end + nl_tokens\n",
    "        input_id += system\n",
    "        # system 不需要拟合\n",
    "        target += im_start + [IGNORE_TOKEN_ID] * (len(system)-3) + im_end + nl_tokens\n",
    "        assert len(input_id) == len(target)\n",
    "        # 依次拼接\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            # user：<|im_start|>human\\ninstruction【EOS】\\n\n",
    "            # assistant：<|im_start|>assistant\\nresponse【EOS】\\n\n",
    "            _input_id = tokenizer(role).input_ids + nl_tokens + \\\n",
    "                tokenizer(sentence[\"value\"]).input_ids + im_end + nl_tokens\n",
    "            input_id += _input_id\n",
    "            if role == '<|im_start|>human':\n",
    "                # user 不需要拟合\n",
    "                _target = im_start + [IGNORE_TOKEN_ID] * (len(_input_id)-3) + im_end + nl_tokens\n",
    "            elif role == '<|im_start|>assistant':\n",
    "                # assistant 需要拟合\n",
    "                _target = im_start + [IGNORE_TOKEN_ID] * len(tokenizer(role).input_ids) + \\\n",
    "                    _input_id[len(tokenizer(role).input_ids)+1:-2] + im_end + nl_tokens\n",
    "            else:\n",
    "                print(role)\n",
    "                raise NotImplementedError\n",
    "            target += _target\n",
    "        assert len(input_id) == len(target)\n",
    "        # 最后进行 PAD\n",
    "        input_id += [tokenizer.pad_token_id] * (max_len - len(input_id))\n",
    "        target += [IGNORE_TOKEN_ID] * (max_len - len(target))\n",
    "        input_ids.append(input_id[:max_len])\n",
    "        targets.append(target[:max_len])\n",
    "    # print(input_ids)\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    targets = torch.tensor(targets)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b3576cb-04d7-448a-9bd1-07cb7b344e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,  ..., 151643, 151643, 151643],\n",
       "         [151644,   8948,    198,  ..., 151643, 151643, 151643]]),\n",
       " 'labels': tensor([[151644, 151643, 151643,  ..., 151643, 151643, 151643],\n",
       "         [151644, 151643, 151643,  ..., 151643, 151643, 151643]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试一下\n",
    "preprocess([lst[0][\"conversations\"],lst[1][\"conversations\"]], tokenizer, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e01dcf-4de4-4470-97dd-3317ef1aa00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义一个 Dataset\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Dict\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer, max_len: int):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        # 加载并预处理数据\n",
    "        sources = [example[\"conversations\"] for example in raw_data[:10000]]\n",
    "        data_dict = preprocess(sources, tokenizer, max_len)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "934316d3-098f-4889-9cb0-d234a630b194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:08<00:00, 1235.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = SupervisedDataset(lst, tokenizer=tokenizer, max_len=2048)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
