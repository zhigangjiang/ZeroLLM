<div align='center'>
    <img src="./docs/images/head.jpg" alt="alt text" width="100%">
    <h1>ZeroLLM</h1>
</div>

![]("./docs/images/head.jpg")
å‚è€ƒæ•™ç¨‹[happy-llm](https://github.com/datawhalechina/happy-llm.git)

ä»0æ­å»ºLLM(åŸºäºLLaMA2)

## æ•°æ®é¢„å¤„ç†
### ä¸‹è½½æ•°æ®
ä¸‹è½½é¢„è®­ç»ƒæ•°æ®

Tokenã€‚
``` bash
bash download/download_pretrain_dataset.sh
```
- å‡ºé—¨é—®é—®åºåˆ—çŒ´å­å¼€æºæ•°æ®é›†ï¼šå‡ºé—¨é—®é—®åºåˆ—çŒ´å­é€šç”¨æ–‡æœ¬æ•°æ®é›†ç”±æ¥è‡ªç½‘é¡µã€ç™¾ç§‘ã€åšå®¢ã€é—®ç­”ã€å¼€æºä»£ç ã€ä¹¦ç±ã€æŠ¥åˆŠã€ä¸“åˆ©ã€æ•™æã€è€ƒé¢˜ç­‰å¤šç§å…¬å¼€å¯è·å–çš„æ•°æ®è¿›è¡Œæ±‡æ€»æ¸…æ´—ä¹‹åè€Œå½¢æˆçš„å¤§è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒè¯­æ–™ã€‚æ€»é‡å¤§æ¦‚åœ¨ 10B Tokenã€‚

- åŒ…å«13000000æ¡æ–‡æœ¬æ•°æ®




ä¸‹è½½SFTè®­ç»ƒæ•°æ®ï¼Œç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼ŒSFT
``` bash
bash download/download_sft_dataset.sh
```
- BelleGroupï¼š350ä¸‡æ¡ä¸­æ–‡å¯¹è¯æ•°æ®é›†ï¼ŒåŒ…å«äº†äººæœºå¯¹è¯ã€äººäººå¯¹è¯ã€äººç‰©å¯¹è¯ç­‰å¤šç§å¯¹è¯æ•°æ®ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒå¯¹è¯ç”Ÿæˆæ¨¡å‹ã€‚

- åŒ…å«3606402æ¡å¤šè½®å¯¹è¯æ•°æ®

### å¤„ç†æ•°æ®
å¤„ç†é¢„è®­ç»ƒæ•°æ®
``` bash
python dataset/pre_process/deal_pretrain_dataset.py
```
æŒ‰chunk_size=512æ‹†åˆ†ï¼Œå¤„ç†åæœ‰28998989æ¡æ–‡æœ¬æ•°æ®

å¤„ç†SFTè®­ç»ƒæ•°æ®
``` bash
python dataset/pre_process/deal_sft_dataset.py
```
å¤„ç†ä¸ºæ ‡æ³¨æ ¼å¼
## è®­ç»ƒTokenizer
> å¯ä»¥ç›´æ¥ä½¿ç”¨è®­ç»ƒå¥½çš„Tokenizerï¼Œä½äº`./tokenizer_k`ï¼Œè·³è¿‡æ­¤æ­¥éª¤

``` bash
python train/train_tokenizer.py
```
## è®­ç»ƒBaseæ¨¡å‹
``` bash
nohup python ./train/pretrain.py --use_swanlab &
```
> åœ¨å•å¡NVIDIA RTX PRO 6000(96GB) ä¸Šï¼Œbatchsizeå¯ä»¥è®¾ç½®åˆ°128ï¼Œå®éªŒé»˜è®¤ä½¿ç”¨64ï¼Œä¸”ä¸ºå¿«é€ŸéªŒè¯ä»…ç”¨`å‡ºé—¨é—®é—®åºåˆ—çŒ´å­å¼€æºæ•°æ®é›†`å‰1000ä¸‡æ¡æ•°æ®ï¼ˆåœ¨`dataset/pretrain_dataset.py#17`ä¿®æ”¹ï¼‰

![pretrain_swanlab.png](docs/images/pretrain_swanlab.png)

è®­ç»ƒå¥½çš„æ¨¡å‹ï¼š[ğŸ¤—modelåœ°å€](https://huggingface.co/zhigangjiang/ZeroLLM/resolve/main/base_model_215M/pretrain_1024_18_6144.pth)

æ¨ç†æµ‹è¯•
``` bash
(llm) root@autodl-container-lhy2360kfm-b38ddd38:~/projects/happy-llm/ZeroLLM# /root/miniconda3/envs/llm/bin/python /root/projects/happy-llm/ZeroLLM/inference/model_sample.py
Model has 215.127 M parameters.

Sample 1:
<|im_start|>åŒ—äº¬å¤§å­¦æ˜¯ä¸­å›½æœ€é«˜å­¦åºœ,ä¹Ÿæ˜¯æ•™è‚²éƒ¨ç›´å±çš„æœ€é«˜å­¦åºœ,åŒ—äº¬å¤§å­¦åœ¨æ•™è‚²éƒ¨æ­¤æ¬¡æ’åä¸­æ’ä½ç¬¬2,å¸ˆèµ„åŠ›é‡ã€ç ”ç©¶æ°´å¹³ã€åŠå­¦å±‚æ¬¡ç­‰å‡å±…å…¨å›½é«˜æ ¡å‰åˆ—ã€‚2019å¹´,åŒ—äº¬å¤§å­¦æ‹›æ”¶38ä¸ªæœ¬ç§‘ä¸“ä¸š,æ‹›æ”¶35ä¸ªç¡•å£«ç ”ç©¶ç”Ÿ,æ‹›æ”¶7ä¸ªåšå£«ç ”ç©¶ç”Ÿ,æ‹›æ”¶17ä¸ªåšå£«åæµåŠ¨ç«™ã€‚2020å¹´,åŒ—äº¬å¤§å­¦æ‹›æ”¶37ä¸ªåšå£«ç ”ç©¶ç”Ÿ
--------------------

Sample 2:
<|im_start|>ä¸­å›½çŸ¿ä¸šå¤§å­¦ï¼ˆåŒ—äº¬ï¼‰åœ°çƒç§‘å­¦ä¸æµ‹ç»˜å·¥ç¨‹å­¦é™¢å‰¯æ•™æˆé»„æ²³è®¤ä¸º,åœ°ä¸‹æ°´å°†æˆä¸ºçŸ³æ²¹å’Œå¤©ç„¶æ°”è¡Œä¸šé‡è¦çš„æˆ˜ç•¥èµ„æºã€‚
æ®äº†è§£,ä¸­å›½çŸ¿ä¸šå¤§å­¦(åŒ—äº¬)åœ°çƒç§‘å­¦ä¸æµ‹ç»˜å·¥ç¨‹å­¦é™¢çš„å¸ˆç”Ÿä»¬æœ‰å¹¸å‚ä¸â€œçŸ¿ä¸š2030â€è®¡åˆ’,åœ¨é»„æ²³æ°´åˆ©å§”å‘˜ä¼šã€ä¸­å›½çŸ³æ²¹å’Œå…¨å›½çŸ³æ²¹å…¬å¸ã€ä¸­å›½çŸ³æ²¹å¤§å­¦(åŒ—äº¬)ã€ä¸­å›½ç§‘å­¦é™¢åœ°çƒç§‘å­¦ç ”ç©¶æ‰€ç­‰å•ä½çš„æœ‰å…³éƒ¨é—¨å’Œå•ä½çš„å¤§åŠ›æ”¯æŒä¸‹,æˆåŠŸå¼€å±•äº†
--------------------
```
## è®­ç»ƒSFTæ¨¡å‹
``` bash
nohup python ./train/sft_train.py --use_swanlab &
```
> åœ¨å•å¡NVIDIA RTX PRO 6000(96GB) ä¸Šï¼Œbatchsizeå¯ä»¥è®¾ç½®åˆ°128ï¼Œå®éªŒé»˜è®¤ä½¿ç”¨64ï¼Œä½¿ç”¨`BelleGroup`å…¨éƒ¨æ•°æ®

![sft_train_swanlab.png](docs/images/sft_train_swanlab.png)

è®­ç»ƒå¥½çš„æ¨¡å‹ï¼š[ğŸ¤—modelåœ°å€](https://huggingface.co/zhigangjiang/ZeroLLM/blob/main/sft_model_215M/sft_dim1024_layers18_vocab_size6144.pth)

æ¨ç†æµ‹è¯•
``` bash
(llm) root@autodl-container-fuwsr34hl4-26a4443c:~/projects/happy-llm/ZeroLLM# /root/miniconda3/envs/llm/bin/python /root/projects/happy-llm/ZeroLLM/inference/model_sample.py

 ------------------- SFT Sample ------------------- 

Model has 215.127 M parameters.

Sample 1:
Question: ä½ å¥½å‘€ 
AI answer: ä½ å¥½,æˆ‘æ˜¯å°æ˜ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—?
--------------------

Sample 2:
Question: ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ 
AI answer: ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ã€‚
--------------------

Sample 3:
Question: 1+12ç­‰äºå¤šå°‘ï¼Ÿ 
AI answer: 1+12ç­‰äº2ã€‚
--------------------

Sample 4:
Question: ä½ æ˜¯è°ï¼Ÿ 
AI answer: æˆ‘æ˜¯ä¸€ä¸ªAIè¯­è¨€æ¨¡å‹,æ²¡æœ‰å…·ä½“çš„åå­—,æ— æ³•å›ç­”ä½ çš„é—®é¢˜ã€‚
--------------------
```
## åŠ è½½æµ‹è¯•Qwen2.5-1.5B
ä¸‹è½½æ¨¡å‹
```bash
bash download/download_Qwen2.5-1.5B.sh
```
æ¨ç†æµ‹è¯•
```bash
(llm) root@autodl-container-fuwsr34hl4-26a4443c:~/projects/happy-llm# /root/miniconda3/envs/llm/bin/python /root/projects/happy-llm/ZeroLLM/inference/model_sample_Qwen2.5-1.5B.py
------------------- Original Pretrain Sample ------------------- 

Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
ä½ å¥½
```
ç”±äºæ²¡æœ‰ç»è¿‡SFTè®­ç»ƒï¼Œå¯¹è¯æ•ˆæœå·®ã€‚

## åŸºäºQwen2.5-1.5B Baseè®­ç»ƒï¼ˆå¾®è°ƒï¼‰
``` bash
bash train/pretrain_Qwen2.5-1.5B.sh
```
æˆ–è€…
``` bash
# è®¾ç½®å¯è§æ˜¾å¡
CUDA_VISIBLE_DEVICES=0

deepspeed train/pretrain_Qwen2.5-1.5B.py \
    --config_name /root/projects/happy-llm/ZeroLLM/autodl-tmp/model/Qwen2.5-1.5B \
    --tokenizer_name /root/projects/happy-llm/ZeroLLM/autodl-tmp/model/Qwen2.5-1.5B \
    --train_files /root/projects/happy-llm/ZeroLLM/autodl-tmp/dataset/seq_monkey_datawhale_small.jsonl \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 4 \
    --do_train \
    --output_dir /root/projects/happy-llm/ZeroLLM/autodl-tmp/model/pretrain_Qwen2.5-1.5B/output \
    --learning_rate 1e-4 \
    --num_train_epochs 1 \
    --warmup_steps 200 \
    --logging_dir /root/projects/happy-llm/ZeroLLM/autodl-tmp/model/pretrain_Qwen2.5-1.5B/logs \
    --logging_strategy steps \
    --logging_steps 5 \
    --save_strategy steps \
    --save_steps 100 \
    --preprocessing_num_workers 10 \
    --save_total_limit 1 \
    --seed 12 \
    --block_size 2048 \
    --bf16 \
    --gradient_checkpointing \
    --deepspeed ./train/ds_config_zero2.json \
    --report_to swanlab \
    # --evaluation_strategy  no \
```
> åœ¨å•å¡NVIDIA RTX PRO 6000(96GB) ä¸Šï¼Œbatchsizeå¯ä»¥è®¾ç½®åˆ°8ï¼Œå®éªŒé»˜è®¤ä½¿ç”¨8ï¼Œä¸”ä¸ºå¿«é€ŸéªŒè¯ä»…ç”¨`å‡ºé—¨é—®é—®åºåˆ—çŒ´å­å¼€æºæ•°æ®é›†`å‰100000æ¡æ•°æ®ï¼ˆåœ¨dataset/pre_process/process_dataset.ipynbä¿®æ”¹ï¼‰


![pretrain_swanlab.png](docs/images/pretrain_Qwen2.5-1.5B_swanlab.png)

è®­ç»ƒå¥½çš„æ¨¡å‹ï¼š[ğŸ¤—modelåœ°å€](https://huggingface.co/zhigangjiang/ZeroLLM/resolve/main/pretrain_Qwen2.5-1.5B)

æ¨ç†æµ‹è¯•
``` bash
(llm) root@autodl-container-fuwsr34hl4-26a4443c:~/projects/happy-llm/ZeroLLM# /root/miniconda3/envs/llm/bin/python /root/projects/happy-llm/ZeroLLM/inference/model_sample_Qwen2.5-1.5B.py
------------------- Pretrain Sample ------------------- 

ï¼Œä»¥â€œä»¥â€œä¸€ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€ä¸‰ã€é‡‘
```
> å¯èƒ½è®­ç»ƒæ•°æ®è¿‡å°‘ï¼Œä¸€æœ¬æ­£ç»ä¹±è¯´ï¼Œ

## åŸºäºQwen2.5-1.5B SFTè®­ç»ƒ
Qwen2.5-1.5Bå·²åŒ…å«è¶³å¤Ÿå¤šçŸ¥è¯†ï¼Œæ‰€ä»¥ç›´æ¥åŸºäºåŸç‰ˆæ¨¡å‹è¿›è¡ŒSFTè®­ç»ƒ

``` bash
bash train/sft_train_Qwen2.5-1.5B.sh
```
æˆ–è€…
``` bash
CUDA_VISIBLE_DEVICES=0,1

deepspeed train/sft_train_Qwen2.5-1.5B.py \
    --model_name_or_path /root/projects/happy-llm/ZeroLLM/autodl-tmp/model/Qwen2.5-1.5B \
    --train_files /root/projects/happy-llm/ZeroLLM/autodl-tmp/dataset/BelleGroup/train_3.5M_CN.json \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 4 \
    --do_train \
    --output_dir  /root/projects/happy-llm/ZeroLLM/autodl-tmp/model/sft_train_Qwen2.5-1.5B/output \
    --learning_rate 1e-4 \
    --num_train_epochs 3 \
    --warmup_steps 200 \
    --logging_dir /root/projects/happy-llm/ZeroLLM/autodl-tmp/model/sft_train_Qwen2.5-1.5B/logs \
    --logging_strategy steps \
    --logging_steps 5 \
    --save_strategy steps \
    --save_steps 100 \
    --save_total_limit 1 \
    --seed 12 \
    --block_size 2048 \
    --bf16 \
    --gradient_checkpointing \
    --deepspeed ./train/ds_config_zero2.json \
    --report_to swanlab \
    # --evaluation_strategy  no \
    # --resume_from_checkpoint ${output_model}/checkpoint-20400 \
```

> åœ¨å•å¡NVIDIA RTX PRO 6000(96GB) ä¸Šï¼Œbatchsizeå¯ä»¥è®¾ç½®åˆ°8ï¼Œå®éªŒé»˜è®¤ä½¿ç”¨8ï¼Œä¸”ä¸ºå¿«é€ŸéªŒè¯ä»…ç”¨`BelleGroup`æ•°æ®é›†å‰10000æ¡æ•°æ®ï¼ˆåœ¨train/sft_train_Qwen2.5-1.5B.py#241ä¿®æ”¹ï¼‰


![pretrain_swanlab.png](docs/images/sft_train_Qwen2.5-1.5B_swanlab.png)
è®­ç»ƒå¥½çš„æ¨¡å‹ï¼š[ğŸ¤—modelåœ°å€](https://huggingface.co/zhigangjiang/ZeroLLM/resolve/main/sft_train_Qwen2.5-1.5B)

æ¨ç†æµ‹è¯•
``` bash
(llm) root@autodl-container-fuwsr34hl4-26a4443c:~/projects/happy-llm# /root/miniconda3/envs/llm/bin/python /root/projects/happy-llm/ZeroLLM/inference/model_sample_Qwen2.5-1.5B.py

 ------------------- SFT Sample ------------------- 

ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ<|im_end|>
<|endoftext|>
```
> å¯ä»¥çœ‹åˆ°è®­ç»ƒåå¯¹æŒ‡ä»¤å›ç­”å‡†ç¡®äº†ï¼Œè¯´æ˜SFTè®­ç»ƒæœ‰æ•ˆæœ